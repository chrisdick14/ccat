legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)
#axis.text.y=element_blank()
) +
coord_flip()
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 5)
#axis.text.y=element_blank()
) +
coord_flip()
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 7)
#axis.text.y=element_blank()
) +
coord_flip()
mod_rf
library(tidyverse)
library(caret)
library(parallel)
library(doParallel)
library(rsample)
library(yardstick)
library(recipes)
library(vip)
library(forcats)
library(albersusa)
library(ggthemes)
set.seed(1992) # Ensure reproducibility
measure <- read_csv("Output/county_main.csv") %>%
select(GEOID, MEASURE = GQ_MEASURE)
evictions <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Raw/final_frame.csv")
evictions <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Output/final_frame.csv")
evictions %>% glimpse
evictions <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Output/final_frame.csv") %>%
select(-d_low_flag, -delta_eviction_rates, -d_above_avg_eviction_rate, -d_state_prohibits_government_sex_discrimination:-d_state_medicaid_expansion, -year, -state_fips:-full_name)
evictions %>% glimpse
vars <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Output/final_frame.csv") %>%
select(-d_low_flag, -delta_eviction_rates, -d_above_avg_eviction_rate, -d_state_prohibits_government_sex_discrimination:-d_state_medicaid_expansion, -year, -state_fips:-full_name, TYPE = urban_type)
vars %>% glimpse
dat <- left_join(measure, vars) %>%
mutate(MEASURE = make.names(MEASURE)) %>%
mutate_if(is.character, as.factor) %>%
mutate(MEASURE = factor(MEASURE, levels = c("Highly.Divergent.from.Expectation", "Slightly.Divergent.from.Expectation", "Close.to.Expectation"))) %>%
select(-GEOID)
vars <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Output/final_frame.csv") %>%
select(-d_low_flag, -delta_eviction_rates, -d_above_avg_eviction_rate, -d_state_prohibits_government_sex_discrimination:-d_state_medicaid_expansion, -year, -state_fips:-full_name, TYPE = urban_type, GEOID = geoid)
dat <- left_join(measure, vars) %>%
mutate(MEASURE = make.names(MEASURE)) %>%
mutate_if(is.character, as.factor) %>%
mutate(MEASURE = factor(MEASURE, levels = c("Highly.Divergent.from.Expectation", "Slightly.Divergent.from.Expectation", "Close.to.Expectation"))) %>%
select(-GEOID)
dat %>% glimpse
vars <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Output/final_frame.csv") %>%
filter(year = 2016) %>%
select(-d_low_flag, -delta_eviction_rates, -d_above_avg_eviction_rate, -d_state_prohibits_government_sex_discrimination:-d_state_medicaid_expansion, -year, -state_fips:-full_name, TYPE = urban_type, GEOID = geoid)
vars <- read_csv("C:/Users/Izzy/GoogleDriveGU/GitHub/capstone/Data/Output/final_frame.csv") %>%
filter(year == 2016) %>%
select(-d_low_flag, -delta_eviction_rates, -d_above_avg_eviction_rate, -d_state_prohibits_government_sex_discrimination:-d_state_medicaid_expansion, -year, -state_fips:-full_name, TYPE = urban_type, GEOID = geoid)
vars %>% glimpse
dat <- left_join(measure, vars) %>%
mutate(MEASURE = make.names(MEASURE)) %>%
mutate_if(is.character, as.factor) %>%
mutate(MEASURE = factor(MEASURE, levels = c("Highly.Divergent.from.Expectation", "Slightly.Divergent.from.Expectation", "Close.to.Expectation"))) %>%
select(-GEOID)
splits <- initial_split(dat,prop = .8,strata = TYPE)
train_data <- training(splits) # Use 80% of the data as training data
test_data <- testing(splits) # holdout 20% as test data
train_data %>% glimpse
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(all_nominal(), -MEASURE) %>%
prep() %>% suppressMessages()
# Apply the recipe to the training and test data
train_data_processed <- suppressMessages(bake(our_recipe,train_data)) %>%
drop_na()
train_data_processed %>% glimpse
test_data_processed <- suppressMessages(bake(our_recipe,test_data))%>%
drop_na()
folds <- createFolds(train_data_processed$MEASURE, k = 5)
# Cross validation settings as an object
control_conditions <-
trainControl(method='cv',
#summaryFunction = twoClassSummary,
classProbs = TRUE,
sampling = "up",
index = folds)
tunegrid <- expand.grid(mtry=11,
splitrule = "gini",
min.node.size = 1)
mod_rf <-
train(MEASURE ~ .,
data=train_data_processed,
method = "ranger",
num.trees = 100,
metric = "ROC",
tuneGrid=tunegrid,
trControl = control_conditions)
mod_rf
obj <- vi_permute(mod_rf, # Machine learning model
train = train_data_processed,
nsim = 10, # Number of times to permute each variable
target = "MEASURE", # outcome
reference_class = "Highly.Divergent.from.Expectation", # what class are you predicting
metric = "accuracy", # metric
pred_wrapper = predict)
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 7)
#axis.text.y=element_blank()
) +
coord_flip()
obj
obj %>% arrange(Importance)
obj %>% arrange(desc(Importance))
# reduced dataset
dat <- dat %>%
select(MEASURE, gdp, density, pct_multiple, average_household_size, pct_hispanic, TYPE)
dat
splits <- initial_split(dat,prop = .8,strata = TYPE)
train_data <- training(splits) # Use 80% of the data as training data
test_data <- testing(splits) # holdout 20% as test data
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(all_nominal(), -MEASURE) %>%
prep() %>% suppressMessages()
# Apply the recipe to the training and test data
train_data_processed <- suppressMessages(bake(our_recipe,train_data)) %>%
drop_na()
train_data
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(all_nominal(), -MEASURE) %>%
prep() %>% suppressMessages()
is.na(train_data)
count(is.na(dat))
count(is.na(train_data))
count(is.na(train_data)$FALSE)
is.na(train_data) %>% count
sum(is.na(train_data))
select(is.na(train_data))
train_data %>%
filter_all(any_vars(! is.na(.)))
train_data %>%
filter_all(any_vars(! is.na(.))) %>% glimpse
train_data %>%
filter_all(any_vars(! is.na(.))) %>% view
train_data %>%
filter_all(any_vars(is.na(.))) %>% view
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(all_nominal(), -MEASURE) %>%
prep() %>% suppressMessages()
train_data
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(TYPE) %>%
prep() %>% suppressMessages()
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
#  step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(TYPE) %>%
prep() %>% suppressMessages()
# Apply the recipe to the training and test data
train_data_processed <- suppressMessages(bake(our_recipe,train_data)) %>%
drop_na()
test_data_processed <- suppressMessages(bake(our_recipe,test_data))%>%
drop_na()
folds <- createFolds(train_data_processed$MEASURE, k = 5)
# Cross validation settings as an object
control_conditions <-
trainControl(method='cv',
#summaryFunction = twoClassSummary,
classProbs = TRUE,
sampling = "up",
index = folds)
tunegrid <- expand.grid(mtry=11,
splitrule = "gini",
min.node.size = 1)
mod_rf <-
train(MEASURE ~ .,
data=train_data_processed,
method = "ranger",
num.trees = 100,
metric = "ROC",
tuneGrid=tunegrid,
trControl = control_conditions)
train_data_processed
mod_rf
mod_rf <-
train(MEASURE ~ .,
data=train_data_processed,
method = "ranger",
num.trees = 100,
metric = "ROC",
tuneGrid=tunegrid,
trControl = control_conditions)
tunegrid <- expand.grid(splitrule = "gini",
min.node.size = 1)
mod_rf <-
train(MEASURE ~ .,
data=train_data_processed,
method = "ranger",
num.trees = 100,
metric = "ROC",
tuneGrid=tunegrid,
trControl = control_conditions)
tunegrid <- expand.grid(mtry=4,
splitrule = "gini",
min.node.size = 1)
mod_rf <-
train(MEASURE ~ .,
data=train_data_processed,
method = "ranger",
num.trees = 100,
metric = "ROC",
tuneGrid=tunegrid,
trControl = control_conditions)
mod_rf
obj <- vi_permute(mod_rf, # Machine learning model
train = train_data_processed,
nsim = 10, # Number of times to permute each variable
target = "MEASURE", # outcome
reference_class = "Highly.Divergent.from.Expectation", # what class are you predicting
metric = "accuracy", # metric
pred_wrapper = predict)
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 7)
#axis.text.y=element_blank()
) +
coord_flip()
splits <- initial_split(dat,prop = .8)
train_data <- training(splits) # Use 80% of the data as training data
# reduced dataset
dat <- dat %>%
select(MEASURE, gdp, density, pct_multiple, average_household_size, pct_hispanic)
splits <- initial_split(dat,prop = .8)
train_data <- training(splits) # Use 80% of the data as training data
test_data <- testing(splits) # holdout 20% as test data
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
#  step_knnimpute(all_numeric()) %>%
step_log(all_numeric(), signed = TRUE) %>%
step_dummy(TYPE) %>%
prep() %>% suppressMessages()
# Apply the recipe to the training and test data
train_data_processed <- suppressMessages(bake(our_recipe,train_data)) %>%
drop_na()
test_data_processed <- suppressMessages(bake(our_recipe,test_data))%>%
drop_na()
train_data
# Apply the recipe to the training and test data
train_data_processed <- suppressMessages(bake(our_recipe,train_data)) %>%
drop_na()
our_recipe <- recipe(MEASURE ~ ., data = train_data) %>%
step_log(all_numeric(), signed = TRUE) %>%
prep() %>% suppressMessages()
# Apply the recipe to the training and test data
train_data_processed <- suppressMessages(bake(our_recipe,train_data)) %>%
drop_na()
test_data_processed <- suppressMessages(bake(our_recipe,test_data))%>%
drop_na()
folds <- createFolds(train_data_processed$MEASURE, k = 5)
# Cross validation settings as an object
control_conditions <-
trainControl(method='cv',
#summaryFunction = twoClassSummary,
classProbs = TRUE,
sampling = "up",
index = folds)
tunegrid <- expand.grid(mtry=4,
splitrule = "gini",
min.node.size = 1)
mod_rf <-
train(MEASURE ~ .,
data=train_data_processed,
method = "ranger",
num.trees = 100,
metric = "ROC",
tuneGrid=tunegrid,
trControl = control_conditions)
mod_rf
obj <- vi_permute(mod_rf, # Machine learning model
train = train_data_processed,
nsim = 10, # Number of times to permute each variable
target = "MEASURE", # outcome
reference_class = "Highly.Divergent.from.Expectation", # what class are you predicting
metric = "accuracy", # metric
pred_wrapper = predict)
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 7)
#axis.text.y=element_blank()
) +
coord_flip()
var_x <- obj %>%
distinct(Variable) %>%
mutate(Variable = as.character(Variable)) %>%
pull()
var_x
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none",
axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1, size = 7)
#axis.text.y=element_blank()
)
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none")
partial(mod_rf, pred.var = "gdp", plot = TRUE,prob=T,
plot.engine = "ggplot2")
partial(mod_rf, pred.var = "gdp", plot = TRUE,prob=T,
plot.engine = "ggplot2")
library(pdp)
partial(mod_rf, pred.var = "gdp", plot = TRUE,prob=T,
plot.engine = "ggplot2")
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none")
partial(mod_rf, pred.var = "pct_hispanic", plot = TRUE,prob=T,
plot.engine = "ggplot2")
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none")
partial(mod_rf, pred.var = "average_household_size", plot = TRUE,prob=T,
plot.engine = "ggplot2")
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none")
partial(mod_rf, pred.var = "pct_multiple", plot = TRUE,prob=T,
plot.engine = "ggplot2")
obj %>%
mutate(Variable = fct_reorder(Variable, Importance, .fun='median')) %>%
ggplot(aes(y=Variable,x=Importance,color=Variable)) +
geom_errorbarh(aes(xmin=Importance-(StDev*2),
xmax=Importance+(StDev*2)),
height =.05,size=1,
position =  position_dodge(width = 0.2)) +
geom_point(size=2,position =  position_dodge(width = 0.2)) +
scale_color_brewer(palette = "RdYlBu") +
theme_minimal() +
labs(title = "Variable Importance",
color="Urban Typology") +
theme(plot.title=element_text(hjust = 0.5,
size = 20,
family = "serif"),
text = element_text(family = "serif",
size = 16),
legend.position="none")
partial(mod_rf, pred.var = "density", plot = TRUE,prob=T,
plot.engine = "ggplot2")
